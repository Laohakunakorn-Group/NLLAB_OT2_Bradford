{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "<br>\n",
    "This notebook is the first step in a workflow that deals with Bradford Protein Quantification 96x well plate reader data:\n",
    "\n",
    "1. Exploratory data analysis.\n",
    "2. Cleaning the data to remove absorbance values that are outside the linear range of the instrument\n",
    "3. Parsing and exporting both the calibrant and sample data into the processed_data_files directory preparatory to downstream analysis\n",
    "\n",
    "Alex Perkins 16th November 2021\n",
    "a.j.p.perkins@sms.ed.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Preparation ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.11.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import pymc3 as pm\n",
    "from pymc3 import HalfCauchy, Model, Normal, glm, plot_posterior_predictive_glm, sample\n",
    "\n",
    "print(f\"Running on PyMC3 v{pm.__version__}\")\n",
    "\n",
    "import arviz as az\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Import curve fitting package from scipy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# import os\n",
    "import os, sys, shutil\n",
    "\n",
    "from experiment_specific_config import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experiment_replicates = len(dilutions_dict[list(dilutions_dict.keys())[0]])\n",
    "num_calibrant_replicates = len(calibrants_dict[list(calibrants_dict.keys())[0]])\n",
    "num_experiment_replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell just deals with reading the data file in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received error: More than 1x .CSV file in the directory\n",
      "['parsed_calibrant_data.csv', '134709_221009_OT2_BRADFORD.csv']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'error_bad_lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m csv_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#import dataset as dataframe\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'error_bad_lines'"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "\n",
    "move_file = False\n",
    "\n",
    "##################################################################\n",
    "# define error handler\n",
    "class UnAcceptedValueError(Exception):   \n",
    "    def __init__(self, data):    \n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return repr(self.data)\n",
    "\n",
    "####################################################################\n",
    "# gets all items in directory\n",
    "items = os.listdir(\".\")\n",
    "\n",
    "# lists all .csv\n",
    "csv_list = []\n",
    "\n",
    "try:\n",
    "    for names in items:\n",
    "        if names.endswith(\".CSV\") | names.endswith(\".csv\"):\n",
    "            csv_list.append(names)\n",
    "\n",
    "except:\n",
    "    print(\"Couldn't find any csv files\")\n",
    "\n",
    "try:\n",
    "    if(len(csv_list) > 1):\n",
    "        raise UnAcceptedValueError(\"More than 1x .CSV file in the directory\");\n",
    "except UnAcceptedValueError as e:\n",
    "    print (\"Received error:\", e.data)\n",
    "    # kills the process\n",
    "    quit()\n",
    "##########################################################################################\n",
    "print(csv_list)\n",
    "\n",
    "\n",
    "\n",
    "experiment_name = csv_list[0]\n",
    "\n",
    "\n",
    "#import dataset as dataframe\n",
    "raw_data = pd.read_csv(csv_list[0], header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bradford_index_list =[]\n",
    "\n",
    "# iterate over the rows\n",
    "for i,row in enumerate(raw_data.iloc[:,0]):\n",
    "    \n",
    "    #if string\n",
    "    if isinstance(row, str):\n",
    "        \n",
    "        #if first 7 characters spell bradford\n",
    "        if row[:8] == \"Bradford\":\n",
    "            # append the row index to the list\n",
    "            bradford_index_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bradford_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = pd.DataFrame(columns=[\"Well\", \"Absorbance\", \"Measurement\"])\n",
    "\n",
    "for first_index in bradford_index_list:\n",
    "    \n",
    "    last_index = first_index + 10\n",
    "    \n",
    "    individual_slice = raw_data.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    for i, row in individual_slice.iterrows():\n",
    "        \n",
    "        \n",
    "        if i >= 2 and i < 10:\n",
    "            \n",
    "            selected_row = individual_slice.iloc[i, :]\n",
    "            \n",
    "            plate_letter_row = selected_row[0]\n",
    "            \n",
    "            data = selected_row[1:-1].reset_index()\n",
    "            \n",
    "            data[\"index\"] = plate_letter_row + data[\"index\"].astype(str)\n",
    "            data[\"measurement\"] = selected_row.iloc[-1]\n",
    "            data = data.set_axis([\"Well\", \"Absorbance\", \"Measurement\"], axis=1, copy=False)\n",
    "            \n",
    "            parsed_data = pd.concat([parsed_data, data], ignore_index=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "#######################################################################\n",
    "\n",
    "print(os.getcwd())\n",
    "path = \"/src/processed_data_files/\"\n",
    "# make directory for sticking the output in\n",
    "if os.path.isdir(path) == False:\n",
    "    os.mkdir(path, mode=0o777)\n",
    "    \n",
    "    \n",
    "# now that the dataset has been read in, move it into the processed_data_files file for neatness.\n",
    "if 1 == move_file:\n",
    "    shutil.move(csv_list[0], path)\n",
    "\n",
    "# navigate into the directory for future processed_data_files storage\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "parsed_data.to_csv(\"unfiltered_parsed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "\n",
    "print(os.getcwd())\n",
    "path = \"/src/output/\"\n",
    "# make directory for sticking the output in\n",
    "if os.path.isdir(path) == False:\n",
    "    os.mkdir(path, mode=0o777)\n",
    "    \n",
    "    \n",
    "# now that the dataset has been read in, move it into the output file for neatness.\n",
    "if 1 == move_file:\n",
    "    shutil.move(csv_list[0], path)\n",
    "\n",
    "# navigate into the directory for future plot storage\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Enviroment Setup ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the raw calibrant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Looking at the raw values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if use_existing_calibrants:\n",
    "\n",
    "    # real concs in wells. Stocks in ug/ml diluted by 20x\n",
    "    calibrant_range = list(calibrants_dict.keys())\n",
    "\n",
    "\n",
    "    calibrant_well_list = []\n",
    "    conc_list = []\n",
    "\n",
    "    for conc, wells in calibrants_dict.items():\n",
    "        calibrant_well_list.extend(wells)\n",
    "        conc_list.extend([conc]*num_calibrant_replicates)\n",
    "\n",
    "    calibrant_df = parsed_data[parsed_data[\"Well\"].isin(calibrant_well_list) & parsed_data[\"Measurement\"].isin([\"Bradford:595\"])]\n",
    "\n",
    "\n",
    "    calibrant_df.loc[:, \"Concentration\"] = conc_list\n",
    "    calibrant_df = calibrant_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_calibrants:\n",
    "    \n",
    "    #######################################################################\n",
    "\n",
    "    print(os.getcwd())\n",
    "    path = \"/src/processed_data_files/\"\n",
    "    # make directory for sticking the output in\n",
    "    if os.path.isdir(path) == False:\n",
    "        os.mkdir(path, mode=0o777)\n",
    "\n",
    "\n",
    "    # navigate into the directory for future processed_data_files storage\n",
    "    os.chdir(path)\n",
    "\n",
    "    calibrant_df.to_csv(\"parsed_calibrant_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_calibrants:\n",
    "    \n",
    "    replicate_wise = pd.DataFrame()\n",
    "\n",
    "    for conc in calibrant_df[\"Concentration\"].unique():\n",
    "\n",
    "        df_s = calibrant_df[calibrant_df[\"Concentration\"] == conc]\n",
    "        abs_ = df_s[\"Absorbance\"]\n",
    "        abs_.index = [\"Rep1\", \"Rep2\", \"Rep3\", \"Rep4\", \"Rep5\"]\n",
    "        replicate_wise = pd.concat([replicate_wise, abs_],axis =1)\n",
    "\n",
    "\n",
    "    replicate_wise = replicate_wise.T\n",
    "    replicate_wise[\"Concentration\"] = calibrant_range\n",
    "\n",
    "    replicate_wise = replicate_wise.reset_index(drop=True)\n",
    "    calibrant_df_reps = replicate_wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Reorganising Calibrant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter the calibrants and only keep those within the linear range  (0.75 - 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step: 4 Defining the absorbance of each concentration as a gaussian.\n",
    "\n",
    "This allows us to store the mean concentration and it's error as a function and calculate the probabilities on demand.\n",
    "\n",
    "Lets assume that the technical error of the absorbance measurements are normally distributed\n",
    "\n",
    "1. We calculate the mean and the standard deviation for each absorbance triplicate.\n",
    "2. We define a gaussian sampling function so we can easily sample and return a granular array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_calibrants:\n",
    "    \n",
    "\n",
    "    # calculate the mean of each triplicate\n",
    "    calibrant_df_reps[\"Mean\"] = calibrant_df_reps[[\"Rep1\", \"Rep2\", \"Rep3\", \"Rep4\", \"Rep5\"]].mean(axis=1)\n",
    "    # calculate the variance of each triplicate\n",
    "    calibrant_df_reps[\"σ\"] = calibrant_df_reps[[\"Rep1\", \"Rep2\", \"Rep3\", \"Rep4\", \"Rep5\"]].iloc[:,:3].std(axis=1)\n",
    "\n",
    "    print(calibrant_df_reps)\n",
    "\n",
    "    calibrants_df_avg = calibrant_df_reps[[\"Mean\", \"σ\"]]\n",
    "\n",
    "\n",
    "    def sample_gaussian(mu, sigma):\n",
    "\n",
    "        # define the x range: mean - 4*sigma and mean + 4*sigma. increments = mean/1000\n",
    "        x = np.arange((mu-(4*sigma)),(mu+(4*sigma)), mu/1000)\n",
    "\n",
    "        # use the norm.pdf (probability density function) to sample and return the array.\n",
    "        return norm.pdf(x, mu, sigma)\n",
    "\n",
    "    # do it for each calibrant. Not currently stored.\n",
    "    for idx, row in calibrants_df_avg.iterrows():\n",
    "        sample_gaussian(calibrant_df_reps.loc[idx][\"Mean\"], calibrant_df_reps.loc[idx][\"σ\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having got our function, lets plot all the gaussians together and have a wee look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Looking at the raw sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real concs in wells. Stocks in ug/ml diluted by 20x\n",
    "dilutions_range = list(dilutions_dict[list(dilutions_dict.keys())[0]].keys())\n",
    "print(dilutions_range)\n",
    "\n",
    "# extract nested dict for first protein mix\n",
    "\n",
    "for i, name in enumerate(list(dilutions_dict.keys())):\n",
    "    \n",
    "    protein_mix = dilutions_dict[list(dilutions_dict.keys())[i]]\n",
    "    protein_name = list(dilutions_dict.keys())[i]\n",
    "    print(name)\n",
    "\n",
    "    dilutions_well_list = []\n",
    "    dilutions_list = []\n",
    "\n",
    "    for dilution, wells in protein_mix.items():\n",
    "        dilutions_well_list.extend(wells)\n",
    "        dilutions_list.extend([dilution]*num_experiment_replicates)\n",
    "\n",
    "    dilutions_df = parsed_data[parsed_data[\"Well\"].isin(dilutions_well_list) & parsed_data[\"Measurement\"].isin([\"Bradford:595\"])]\n",
    "\n",
    "\n",
    "    dilutions_df.loc[:, \"DilutionX\"] = dilutions_list\n",
    "    dilutions_df.loc[:, \"ProteinMix\"] = protein_name\n",
    "\n",
    "    dilutions_df = dilutions_df.reset_index(drop=True)\n",
    "dilutions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "\n",
    "print(os.getcwd())\n",
    "path = \"/src/processed_data_files/\"\n",
    "# make directory for sticking the output in\n",
    "if os.path.isdir(path) == False:\n",
    "    os.mkdir(path, mode=0o777)\n",
    "    \n",
    "\n",
    "# navigate into the directory for future processed_data_files storage\n",
    "os.chdir(path)\n",
    "\n",
    "dilutions_df.to_csv(\"tidy_sample_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_wise = pd.DataFrame()\n",
    "\n",
    "for conc in dilutions_df[\"DilutionX\"].unique():\n",
    "    \n",
    "    df_s = dilutions_df[dilutions_df[\"DilutionX\"] == conc]\n",
    "    abs_ = df_s[\"Absorbance\"]\n",
    "    abs_.index = [\"Rep1\", \"Rep2\", \"Rep3\"]\n",
    "    replicate_wise = pd.concat([replicate_wise, abs_],axis =1)\n",
    "\n",
    "replicate_wise = replicate_wise.T\n",
    "print(replicate_wise)\n",
    "print(dilutions_range)\n",
    "replicate_wise[\"DilutionX\"] = dilutions_range\n",
    "\n",
    "replicate_wise = replicate_wise.reset_index(drop=True)\n",
    "dilutions_df_reps = replicate_wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilutions_df_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate the mean of each triplicate\n",
    "dilutions_df_reps[\"Mean\"] = dilutions_df_reps[[\"Rep1\", \"Rep2\", \"Rep3\"]].mean(axis=1)\n",
    "# calculate the variance of each triplicate\n",
    "dilutions_df_reps[\"σ\"] = dilutions_df_reps[[\"Rep1\", \"Rep2\", \"Rep3\"]].iloc[:,:3].std(axis=1)\n",
    "\n",
    "print(dilutions_df_reps)\n",
    "\n",
    "dilutions_df_avg = dilutions_df_reps[[\"Mean\", \"σ\"]]\n",
    "\n",
    "\n",
    "def sample_gaussian(mu, sigma):\n",
    "    \n",
    "    # define the x range: mean - 4*sigma and mean + 4*sigma. increments = mean/1000\n",
    "    x = np.arange((mu-(4*sigma)),(mu+(4*sigma)), mu/1000)\n",
    "    \n",
    "    # use the norm.pdf (probability density function) to sample and return the array.\n",
    "    return norm.pdf(x, mu, sigma)\n",
    "\n",
    "# do it for each calibrant. Not currently stored.\n",
    "for idx, row in dilutions_df_avg.iterrows():\n",
    "    sample_gaussian(dilutions_df_reps.loc[idx][\"Mean\"], dilutions_df_reps.loc[idx][\"σ\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make directory for sticking the heat maps into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "path = \"/src/output/sampleheatmaps/\"\n",
    "\n",
    "# make directory for sticking the output in\n",
    "if os.path.isdir(path) == False:\n",
    "    os.mkdir(path, mode=0o777)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the parsed calibrant and sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check if '/processed_data_files' exists. If not, create it and navigate inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "path_processed_data = \"/src/processed_data_files/\"\n",
    "# make directory for sticking the processed data in\n",
    "if os.path.isdir(path_processed_data) == False:\n",
    "    os.mkdir(path_processed_data, mode=0o777)\n",
    "\n",
    "# navigate into the path_processed_data directory for data storage\n",
    "os.chdir(path_processed_data)\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Export both calibrant and sample data into processed_data_files as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigate back into the root directory for neatness\n",
    "os.chdir(\"/src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
